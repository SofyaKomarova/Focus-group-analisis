{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - nltk\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    conda-4.11.0               |   py38hecd8cb5_0        14.4 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        14.4 MB\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  conda                               4.10.3-py38hecd8cb5_0 --> 4.11.0-py38hecd8cb5_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "conda-4.11.0         | 14.4 MB   | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c anaconda nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ad', 'al', 'allo', 'ai', 'agli', 'all', 'agl', 'alla', 'alle', 'con', 'col', 'coi', 'da', 'dal', 'dallo', 'dai', 'dagli', 'dall', 'dagl', 'dalla', 'dalle', 'di', 'del', 'dello', 'dei', 'degli', 'dell', 'degl', 'della', 'delle', 'in', 'nel', 'nello', 'nei', 'negli', 'nell', 'negl', 'nella', 'nelle', 'su', 'sul', 'sullo', 'sui', 'sugli', 'sull', 'sugl', 'sulla', 'sulle', 'per', 'tra', 'contro', 'io', 'tu', 'lui', 'lei', 'noi', 'voi', 'loro', 'mio', 'mia', 'miei', 'mie', 'tuo', 'tua', 'tuoi', 'tue', 'suo', 'sua', 'suoi', 'sue', 'nostro', 'nostra', 'nostri', 'nostre', 'vostro', 'vostra', 'vostri', 'vostre', 'mi', 'ti', 'ci', 'vi', 'lo', 'la', 'li', 'le', 'gli', 'ne', 'il', 'un', 'uno', 'una', 'ma', 'ed', 'se', 'perché', 'anche', 'come', 'dov', 'dove', 'che', 'chi', 'cui', 'non', 'più', 'quale', 'quanto', 'quanti', 'quanta', 'quante', 'quello', 'quelli', 'quella', 'quelle', 'questo', 'questi', 'questa', 'queste', 'si', 'tutto', 'tutti', 'a', 'c', 'e', 'i', 'l', 'o', 'ho', 'hai', 'ha', 'abbiamo', 'avete', 'hanno', 'abbia', 'abbiate', 'abbiano', 'avrò', 'avrai', 'avrà', 'avremo', 'avrete', 'avranno', 'avrei', 'avresti', 'avrebbe', 'avremmo', 'avreste', 'avrebbero', 'avevo', 'avevi', 'aveva', 'avevamo', 'avevate', 'avevano', 'ebbi', 'avesti', 'ebbe', 'avemmo', 'aveste', 'ebbero', 'avessi', 'avesse', 'avessimo', 'avessero', 'avendo', 'avuto', 'avuta', 'avuti', 'avute', 'sono', 'sei', 'è', 'siamo', 'siete', 'sia', 'siate', 'siano', 'sarò', 'sarai', 'sarà', 'saremo', 'sarete', 'saranno', 'sarei', 'saresti', 'sarebbe', 'saremmo', 'sareste', 'sarebbero', 'ero', 'eri', 'era', 'eravamo', 'eravate', 'erano', 'fui', 'fosti', 'fu', 'fummo', 'foste', 'furono', 'fossi', 'fosse', 'fossimo', 'fossero', 'essendo', 'faccio', 'fai', 'facciamo', 'fanno', 'faccia', 'facciate', 'facciano', 'farò', 'farai', 'farà', 'faremo', 'farete', 'faranno', 'farei', 'faresti', 'farebbe', 'faremmo', 'fareste', 'farebbero', 'facevo', 'facevi', 'faceva', 'facevamo', 'facevate', 'facevano', 'feci', 'facesti', 'fece', 'facemmo', 'faceste', 'fecero', 'facessi', 'facesse', 'facessimo', 'facessero', 'facendo', 'sto', 'stai', 'sta', 'stiamo', 'stanno', 'stia', 'stiate', 'stiano', 'starò', 'starai', 'starà', 'staremo', 'starete', 'staranno', 'starei', 'staresti', 'starebbe', 'staremmo', 'stareste', 'starebbero', 'stavo', 'stavi', 'stava', 'stavamo', 'stavate', 'stavano', 'stetti', 'stesti', 'stette', 'stemmo', 'steste', 'stettero', 'stessi', 'stesse', 'stessimo', 'stessero', 'stando', 'però', 'ecco', 'ee', 'quindi', 'eh', 'Eh', 'cioè', 'ma', 'quel', 'allora', 'ah', 'sì', 'si.', 'si', 'eh sì', 'sì ma', 'va bene', 'anch io', 'allora', 'io', 'anch', 'Allora', 'E', 'io', 'Ah ', 'Eh\\t', 'Va ', 'bene', 'Ma', 'anche', 'Però', 'però', 'ecco', 'ee', 'quindi', 'eh', 'Eh', 'cioè', 'ma', 'quel', 'allora', 'ah', 'sì', 'si.', 'si', 'eh sì', 'sì ma', 'va bene', 'anch io', 'allora', 'io', 'anch', 'Allora', 'E', 'io', 'Ah ', 'Eh\\t', 'Va ', 'bene', 'Ma', 'anche', 'Però', 'no', 'po', 'Poi', 'possono', 'e', 'appunto', 'me', 'invece', 'può', 'potrebbe', 'Quindi', 'Ah', 'adesso', 'Per', 'sì', 'magari', 'insomma', 'cè', 'cosa', 'Un', 'Sì', 'Non', 'Cioè', 'Che', 'proprio', 'Il', 'EE', 'di', 'che', 'Si', 'Perché', 'No', 'Eh', 'ecco', 'a', 'Però', 'Io', 'Ha', 'Di', 'A', 'te', 'lì', 'io', 'In', 'per', 'dei', 'Se', 'La', 'Anche', 'AA', 'È', 'ma', 'le', 'la', 'già', 'eh', 'do', 'cioè', 'Questo', 'Poi', 'Le', 'Cè', 'è', 'un', 'sé', 'si', 'se', 'sai', 'qui', 'più', 'noi', 'mi', 'li', 'ha', 'ce', 'anchio', 'Una', 'molto', 'molta', 'molti', 'moltissimo', 'poter', 'potrei', 'potrebbe', 'potrebbero', 'essere', 'potevano', 'potevo', 'poteva', 'potersi', 'poterlo', 'poterlo', 'potere', 'poter', 'potuto', 'possono', 'posso', 'poter', 'potrei', 'potrebbero', 'potrebbe', 'potevo', 'potevano', 'poteva', 'potersi', 'poterlo', 'potere', 'poter', 'potuto', 'possono', 'posso', 'Po', 'interessante', 'sì', 'ma', 'anch', 'io', 'dell', 'sempre', 'pensavo', 'pensare', 'tutte', 'poco', 'dopo', 'prima', 'potrebbe', 'potere', 'potevano', 'attimo']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('italian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "# word_tokenize accepts\n",
    "# a string as an input, not a file.\n",
    "stop_words = set(stopwords.words('italian'))\n",
    "file1 = open(\"filteredtext620.txt\")\n",
    " \n",
    "# Use this to read file content as a stream:\n",
    "line = file1.read()\n",
    "words = line.split()\n",
    "for r in words:\n",
    "    if not r in stop_words:\n",
    "        appendFile = open('filteredtext623.txt','a')\n",
    "        appendFile.write(\" \"+r)\n",
    "        appendFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('italian')\n",
    "newStopWords = ['ecco','quel']\n",
    "stopwords.extend(newStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('italian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('italian')\n",
    "new_words=('ecco','quel')\n",
    "for i in new_words:\n",
    "    stopwords.append(i)\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "# word_tokenize accepts\n",
    "# a string as an input, not a file.\n",
    "stop_words = set(stopwords.words('italian'))\n",
    "file1 = open(\"filteredtext619.txt\")\n",
    " \n",
    "# Use this to read file content as a stream:\n",
    "line = file1.read()\n",
    "words = line.split()\n",
    "for r in words:\n",
    "    if not r in stop_words:\n",
    "        appendFile = open('filteredtext106.txt','a')\n",
    "        appendFile.write(\" \"+r)\n",
    "        appendFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#...#\n",
    "stop_words = set(stopwords.words(\"italian\"))\n",
    "\n",
    "#add words that aren't in the NLTK stopwords list\n",
    "new_stopwords = ['ecco', 'quel', 'però']\n",
    "new_stopwords_list = stop_words.union(new_stopwords)\n",
    "\n",
    "print(new_stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "# word_tokenize accepts\n",
    "# a string as an input, not a file.\n",
    "stop_words = set(stopwords.words('italian'))\n",
    "file1 = open(\"filteredtext9.txt\")\n",
    " \n",
    "# Use this to read file content as a stream:\n",
    "line = file1.read()\n",
    "words = line.split()\n",
    "for r in words:\n",
    "    if not r in stop_words:\n",
    "        appendFile = open('filteredtext15.txt','a')\n",
    "        appendFile.write(\" \"+r)\n",
    "        appendFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
